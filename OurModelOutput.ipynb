{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_query_length = 15\n",
    "max_category_length = 45\n",
    "num_of_classes = 5\n",
    "epochs = 1000\n",
    "batch_size = 8192\n",
    "checkpoint = '/data/shpx/data/mkabir/checkpoint/1000_train_sampling_zero_point_one_two_tower_model.h5'\n",
    "n_tr_rows = None\n",
    "n_ts_rows = None\n",
    "n_vl_rows = None\n",
    "result_path = '/data/shpx/data/mkabir/train_sample_output_0point1_1000.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"/data/shpx/data/mkabir/dataset/train.tsv\", sep = \"\\t\", header = None, nrows = n_tr_rows)\n",
    "df_train = df_train.dropna(how='any')\n",
    "df_train = df_train.sample(frac=0.1, replace=True, random_state=1)\n",
    "df_val = pd.read_csv(\"/data/shpx/data/mkabir/dataset/dev.tsv\", sep = \"\\t\", header = None, nrows = n_ts_rows)\n",
    "df_val = df_val.dropna(how='any')\n",
    "df_test = pd.read_csv(\"/data/shpx/data/mkabir/dataset/test.tsv\", sep = \"\\t\", header = None, nrows = n_vl_rows)\n",
    "df_test = df_test.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "818428\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(df_train.values[:,0])\n",
    "t.fit_on_texts(df_train.values[:,1])\n",
    "t.fit_on_texts(df_val.values[:,0])\n",
    "t.fit_on_texts(df_val.values[:,1])\n",
    "t.fit_on_texts(df_test.values[:,0])\n",
    "t.fit_on_texts(df_test.values[:,1])\n",
    "t.fit_on_texts(['all categories'])\n",
    "\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "print(len(t.word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176352\n",
      "Loaded 176096 word vectors.\n"
     ]
    }
   ],
   "source": [
    "with open(\"/data/shpx/data/mkabir/dataset/jose_w3_queries_w_singularity_cats.txt\", encoding='utf-8') as f:\n",
    "    content = f.readlines()\n",
    "print(len(content))\n",
    "embeddings_index = {}  \n",
    "for c in content[1:]:\n",
    "    data = c.split(\" \")\n",
    "    #print(len(data))\n",
    "    if len(data) == 102:\n",
    "        key = data[0].lower()\n",
    "        key = key.replace('Ã©', 'e')\n",
    "        embeddings_index[key] = np.array([float(a) for a in data[1:101]])\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "#print(list(embeddings_index.items())[:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_labels(val_y):\n",
    "    lab_y = []\n",
    "    thresholds = [-1.1, 0.82, 0.9, 0.95, 0.97, 1]\n",
    "    \n",
    "    for v in val_y:\n",
    "        l = 0\n",
    "        start = thresholds[0]\n",
    "        p = 1 - v\n",
    "        for thr in thresholds[1:]:\n",
    "            if start < p and p <= thr:\n",
    "                lab_y.append(l)\n",
    "                break\n",
    "            if p > 1:\n",
    "                lab_y.append(4)\n",
    "                break\n",
    "            l = l + 1\n",
    "            start = thr\n",
    "    return lab_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First step completed\n"
     ]
    }
   ],
   "source": [
    "q1 = []\n",
    "c1 = []\n",
    "q2 = []\n",
    "c2 = []\n",
    "for v in df_train.values:\n",
    "    splitted = v[0].split('<::>')\n",
    "    if len(splitted) >= 2:\n",
    "        q1.append(splitted[0])\n",
    "        c1.append(splitted[1])\n",
    "    else:\n",
    "        q1.append(splitted[0])\n",
    "        c1.append('all categories')\n",
    "        \n",
    "    splitted = v[1].split('<::>')\n",
    "    if len(splitted) >= 2:\n",
    "        q2.append(splitted[0])\n",
    "        c2.append(splitted[1])\n",
    "    else:\n",
    "        q2.append(splitted[0])\n",
    "        c2.append('all categories')\n",
    "\n",
    "    \n",
    "q1 = t.texts_to_sequences(q1)\n",
    "q1 = pad_sequences(q1, maxlen=max_query_length, padding='post')\n",
    "c1 = t.texts_to_sequences(c1)\n",
    "c1 = pad_sequences(c1, maxlen=max_category_length, padding='post')\n",
    "\n",
    "print('First step completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second step completed\n"
     ]
    }
   ],
   "source": [
    "q2 = t.texts_to_sequences(q2)\n",
    "q2 = pad_sequences(q2, maxlen=max_query_length, padding='post')\n",
    "c2 = t.texts_to_sequences(c2)\n",
    "c2 = pad_sequences(c2, maxlen=max_category_length, padding='post')\n",
    "print('Second step completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49524536, 15)\n",
      "(49524536,)\n",
      "[array([[ 1030,  7141,  6218, ...,     0,     0,     0],\n",
      "       [ 1877,  7547,    53, ...,     0,     0,     0],\n",
      "       [32661,   406,  2595, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  654, 23200,  7703, ...,     0,     0,     0],\n",
      "       [  297,  5698,  5807, ...,     0,     0,     0],\n",
      "       [ 9126,  1883,    82, ...,     0,     0,     0]], dtype=int32), array([[ 888, 2003,    1, ...,    0,    0,    0],\n",
      "       [  14,   21,    1, ...,    0,    0,    0],\n",
      "       [  97,   79,   75, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [  19,   12,    6, ...,    0,    0,    0],\n",
      "       [ 111,  225, 1026, ...,    0,    0,    0],\n",
      "       [   9,   11,   42, ...,    0,    0,    0]], dtype=int32), array([[ 6218,     0,     0, ...,     0,     0,     0],\n",
      "       [ 7547,   930,     0, ...,     0,     0,     0],\n",
      "       [  538,     0,     0, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [ 3252,  1264,     0, ...,     0,     0,     0],\n",
      "       [  318,  5698,  5807, ...,     0,     0,     0],\n",
      "       [12577,  1447,     0, ...,     0,     0,     0]], dtype=int32), array([[   888,   2003,      1, ...,      0,      0,      0],\n",
      "       [    14,     21,      1, ...,      0,      0,      0],\n",
      "       [   248, 333595,      0, ...,      0,      0,      0],\n",
      "       ...,\n",
      "       [    15,     13,      6, ...,      0,      0,      0],\n",
      "       [   111,    225,   1026, ...,      0,      0,      0],\n",
      "       [     9,     11,     42, ...,      0,      0,      0]], dtype=int32)]\n",
      "[2 4 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "train_x = [np.array(q1.copy()),np.array(c1.copy()), np.array(q2.copy()), np.array(c2.copy())]\n",
    "train_y = np.array(calculate_labels(df_train.values[:,2]))\n",
    "print(train_x[0].shape)\n",
    "print(train_y.shape)\n",
    "\n",
    "print(train_x[:5])\n",
    "print(train_y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5610865, 15)\n",
      "(5610865,)\n"
     ]
    }
   ],
   "source": [
    "q1 = []\n",
    "c1 = []\n",
    "q2 = []\n",
    "c2 = []\n",
    "for v in df_val.values:\n",
    "    splitted = v[0].split('<::>')\n",
    "    if len(splitted) >= 2:\n",
    "        q1.append(splitted[0])\n",
    "        c1.append(splitted[1])\n",
    "    else:\n",
    "        q1.append(splitted[0])\n",
    "        c1.append('all categories')\n",
    "        \n",
    "    splitted = v[1].split('<::>')\n",
    "    if len(splitted) >= 2:\n",
    "        q2.append(splitted[0])\n",
    "        c2.append(splitted[1])\n",
    "    else:\n",
    "        q2.append(splitted[0])\n",
    "        c2.append('all categories')\n",
    "        \n",
    "    \n",
    "q1 = t.texts_to_sequences(q1)\n",
    "q1 = pad_sequences(q1, maxlen=max_query_length, padding='post')\n",
    "c1 = t.texts_to_sequences(c1)\n",
    "c1 = pad_sequences(c1, maxlen=max_category_length, padding='post')\n",
    "\n",
    "q2 = t.texts_to_sequences(q2)\n",
    "q2 = pad_sequences(q2, maxlen=max_query_length, padding='post')\n",
    "c2 = t.texts_to_sequences(c2)\n",
    "c2 = pad_sequences(c2, maxlen=max_category_length, padding='post')\n",
    "val_x = [np.array(q1.copy()),np.array(c1.copy()), np.array(q2.copy()), np.array(c2.copy())]\n",
    "val_y = np.array(calculate_labels(df_val.values[:,2]))\n",
    "print(val_x[0].shape)\n",
    "print(val_y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8159537, 15)\n",
      "(8159537,)\n"
     ]
    }
   ],
   "source": [
    "q1 = []\n",
    "c1 = []\n",
    "q2 = []\n",
    "c2 = []\n",
    "for v in df_test.values:\n",
    "    splitted = v[0].split('<::>')\n",
    "    if len(splitted) >= 2:\n",
    "        q1.append(splitted[0])\n",
    "        c1.append(splitted[1])\n",
    "    else:\n",
    "        q1.append(splitted[0])\n",
    "        c1.append('all categories')\n",
    "        splitted = v[0].split('<::>')\n",
    "        \n",
    "    splitted = v[1].split('<::>')\n",
    "    if len(splitted) >= 2:\n",
    "        q2.append(splitted[0])\n",
    "        c2.append(splitted[1])\n",
    "    else:\n",
    "        q2.append(splitted[0])\n",
    "        c2.append('all categories')\n",
    "        \n",
    "\n",
    "q1 = t.texts_to_sequences(q1)\n",
    "q1 = pad_sequences(q1, maxlen=max_query_length, padding='post')\n",
    "c1 = t.texts_to_sequences(c1)\n",
    "c1 = pad_sequences(c1, maxlen=max_category_length, padding='post')\n",
    "\n",
    "q2 = t.texts_to_sequences(q2)\n",
    "q2 = pad_sequences(q2, maxlen=max_query_length, padding='post')\n",
    "c2 = t.texts_to_sequences(c2)\n",
    "c2 = pad_sequences(c2, maxlen=max_category_length, padding='post')\n",
    "\n",
    "\n",
    "test_x = [np.array(q1.copy()),np.array(c1.copy()), np.array(q2.copy()), np.array(c2.copy())]\n",
    "test_y = np.array(calculate_labels(df_test.values[:,2]))\n",
    "print(test_x[0].shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "exception = 0\n",
    "words = []\n",
    "for word, i in t.word_index.items():\n",
    "    w = word.lower()\n",
    "    w = w.replace('Ã©', 'e')\n",
    "    embedding_vector = embeddings_index.get(w)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        embedding_matrix[i] = np.zeros(100)\n",
    "        words.append(w)\n",
    "        exception = exception + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_input = Input(shape=[max_query_length], name='query1')\n",
    "c1_input = Input(shape=[max_category_length], name='cat1')\n",
    "q2_input = Input(shape=[max_query_length], name='query2')\n",
    "c2_input = Input(shape=[max_category_length], name='cat2')\n",
    "\n",
    "q_emb = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_query_length, trainable=False, mask_zero = True)\n",
    "c_emb = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_category_length, trainable=False, mask_zero = True)\n",
    "\n",
    "q1_embedding = q_emb(q1_input)\n",
    "c1_embedding = c_emb(c1_input)\n",
    "q2_embedding = q_emb(q2_input)\n",
    "c2_embedding = c_emb(c2_input)\n",
    "\n",
    "q1_avg = GlobalAveragePooling1D()(q1_embedding)\n",
    "c1_avg = GlobalAveragePooling1D()(c1_embedding)\n",
    "q2_avg = GlobalAveragePooling1D()(q2_embedding)\n",
    "c2_avg = GlobalAveragePooling1D()(c2_embedding)\n",
    "\n",
    "input_vecs = Concatenate()([q1_avg, c1_avg, q2_avg, c2_avg])\n",
    "\n",
    "x = Dense(256, activation=\"relu\")(input_vecs) \n",
    "\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "x = Dense(128, activation=\"relu\")(x) \n",
    "\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "y = Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "check = Model(inputs = c1_input, outputs=c1_avg)\n",
    "\n",
    "model = Model(inputs=[q1_input, c1_input, q2_input, c2_input], outputs=y)\n",
    "model.compile(loss= 'mse', optimizer = 'adam', metrics=['mse', 'mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = ModelCheckpoint(checkpoint, monitor='val_loss', verbose=1, save_best_only = True)\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 2000, verbose=1, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 1030,  7141,  6218, ...,     0,     0,     0],\n",
      "       [ 1877,  7547,    53, ...,     0,     0,     0],\n",
      "       [32661,   406,  2595, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  654, 23200,  7703, ...,     0,     0,     0],\n",
      "       [  297,  5698,  5807, ...,     0,     0,     0],\n",
      "       [ 9126,  1883,    82, ...,     0,     0,     0]], dtype=int32), array([[ 888, 2003,    1, ...,    0,    0,    0],\n",
      "       [  14,   21,    1, ...,    0,    0,    0],\n",
      "       [  97,   79,   75, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [  19,   12,    6, ...,    0,    0,    0],\n",
      "       [ 111,  225, 1026, ...,    0,    0,    0],\n",
      "       [   9,   11,   42, ...,    0,    0,    0]], dtype=int32), array([[ 6218,     0,     0, ...,     0,     0,     0],\n",
      "       [ 7547,   930,     0, ...,     0,     0,     0],\n",
      "       [  538,     0,     0, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [ 3252,  1264,     0, ...,     0,     0,     0],\n",
      "       [  318,  5698,  5807, ...,     0,     0,     0],\n",
      "       [12577,  1447,     0, ...,     0,     0,     0]], dtype=int32), array([[   888,   2003,      1, ...,      0,      0,      0],\n",
      "       [    14,     21,      1, ...,      0,      0,      0],\n",
      "       [   248, 333595,      0, ...,      0,      0,      0],\n",
      "       ...,\n",
      "       [    15,     13,      6, ...,      0,      0,      0],\n",
      "       [   111,    225,   1026, ...,      0,      0,      0],\n",
      "       [     9,     11,     42, ...,      0,      0,      0]], dtype=int32)]\n"
     ]
    }
   ],
   "source": [
    "print(train_x[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6046/6046 [==============================] - 434s 72ms/step - loss: 0.7560 - mse: 0.7560 - mae: 0.6669 - val_loss: 0.5825 - val_mse: 0.5825 - val_mae: 0.5789\n"
     ]
    }
   ],
   "source": [
    "#history = model.fit(train_x, train_y, validation_data = (val_x, val_y), epochs = epochs, batch_size = batch_size,callbacks = [es, mc])\n",
    "history = model.fit(train_x, train_y, validation_data = (val_x, val_y), epochs = 1, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_labels_from_predictions(val_y, num_of_classes):\n",
    "    lab_y = []\n",
    "    \n",
    "    for v in val_y:\n",
    "        if np.isnan(v):\n",
    "            lab_y.append(num_of_classes - 1)\n",
    "            print('Still None')\n",
    "            continue\n",
    "        t = round(v[0])\n",
    "        if t < 0:\n",
    "            t = 0\n",
    "        if t > num_of_classes - 1:\n",
    "            t = num_of_classes - 1\n",
    "        lab_y.append(t)\n",
    "    return lab_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model.predict(test_x)\n",
    "num_of_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8588687 ]\n",
      " [1.1846895 ]\n",
      " [0.22063911]\n",
      " [3.7653232 ]\n",
      " [1.1964676 ]\n",
      " [2.9128995 ]\n",
      " [1.9520273 ]\n",
      " [0.4687314 ]\n",
      " [0.09857488]\n",
      " [0.31484908]]\n",
      "[False]\n"
     ]
    }
   ],
   "source": [
    "print(pred_y[:10])\n",
    "print(np.isnan(pred_y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = calculate_labels_from_predictions(pred_y, num_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(test_y, pred_y)\n",
    "male = np.sum([abs(pred_y[i] - a) for i,a in enumerate(test_y)]) / len(test_y)\n",
    "with open(result_path, 'w') as f:\n",
    "    for item in [acc, male]:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5881182964180444, 0.45956173738779543]\n"
     ]
    }
   ],
   "source": [
    "print([acc,male])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8159537, 8159537]\n"
     ]
    }
   ],
   "source": [
    "print([len(test_y), len(pred_y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
